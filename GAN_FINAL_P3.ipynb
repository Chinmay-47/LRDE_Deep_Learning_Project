{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_FINAL_P3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chinmay-47/LRDE_Deep_Learning_Project/blob/master/GAN_FINAL_P3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DweJmsRRCAlk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6f45c33f-8530-4ba6-b32c-96fb742b084d"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, LeakyReLU, MaxPool2D\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D, LayerNormalization\n",
        "# from tensorflow.keras.layers.advanced_activations import LeakyReLU\n",
        "from tensorflow.keras.layers import UpSampling2D, Conv2D, Conv2DTranspose\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import sys\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Comprhensive mode failure at around 2000 epochs\n",
        "\n",
        "class DCGAN():\n",
        "    def __init__(self):\n",
        "        # Input shape\n",
        "        self.img_rows = 64\n",
        "        self.img_cols = 64\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.latent_dim = 32\n",
        "        self.weight_decay = 1e-4\n",
        "\n",
        "        optimizer = Adam(0.0001 , 0.5)\n",
        "        optimizer1 = Adam(0.0001 , 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss='binary_crossentropy',\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # The generator takes noise as input and generates imgs\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        img = self.generator(z)\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # The discriminator takes generated images as input and determines validity\n",
        "        valid = self.discriminator(img)\n",
        "\n",
        "        # The combined model  (stacked generator and discriminator\n",
        "        # Trains the generator to fool the discriminator\n",
        "        self.combined = Model(z, valid)\n",
        "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer1)\n",
        "\n",
        "    def build_generator(self):\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Dense(4*4*128 , activation = 'linear' , input_dim = self.latent_dim))\n",
        "        model.add(Reshape((4,4,128)))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(LeakyReLU(alpha = 0.2))\n",
        "        model.add(UpSampling2D()) \n",
        "        model.add(Conv2DTranspose(filters = 64 , kernel_size = 5 , padding = 'same'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(LeakyReLU(alpha = 0.2))\n",
        "        model.add(UpSampling2D())\n",
        "        model.add(Conv2DTranspose(filters = 32 , kernel_size = 5 , padding = 'same'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(LeakyReLU(alpha = 0.2))\n",
        "        model.add(UpSampling2D()) \n",
        "        model.add(Conv2DTranspose(filters = 16 , kernel_size = 5 , padding = 'same'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(LeakyReLU(alpha = 0.2))\n",
        "        model.add(UpSampling2D()) \n",
        "        model.add(Conv2DTranspose(filters = 8 , kernel_size = 5 , padding = 'same'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(LeakyReLU(alpha = 0.2))\n",
        "        model.add(Conv2D(filters = 8 , kernel_size = 5 , padding = 'same'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(LeakyReLU(alpha = 0.2)) \n",
        "        model.add(Conv2D(filters = 4 , kernel_size = 5 , padding = 'same'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(LeakyReLU(alpha = 0.2)) \n",
        "        model.add(Conv2D(filters = 1 , kernel_size = 5 , padding = 'same'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(LeakyReLU(alpha = 0.2)) \n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        img = model(noise)\n",
        "\n",
        "        return Model(noise, img)\n",
        "\n",
        "    def build_discriminator(self):\n",
        "        model = Sequential()\n",
        "        # model.add(Conv2DTranspose(4, kernel_size=5, input_shape=self.img_shape, padding=\"same\"))\n",
        "        # model.add(BatchNormalization()) ##\n",
        "        # model.add(LeakyReLU(alpha = 0.2))\n",
        "        # model.add(Conv2DTranspose(8, kernel_size=5, input_shape=self.img_shape, padding=\"same\"))\n",
        "        # model.add(BatchNormalization()) ##\n",
        "        # model.add(LeakyReLU(alpha = 0.2))\n",
        "        # model.add(Conv2D(8, kernel_size=5, input_shape=self.img_shape, padding=\"same\"))\n",
        "        # model.add(BatchNormalization()) ##\n",
        "        # model.add(LeakyReLU(alpha = 0.2))\n",
        "        model.add(Conv2D(16, kernel_size=5, input_shape=self.img_shape, padding=\"same\"))\n",
        "        model.add(BatchNormalization()) #\n",
        "        model.add(LeakyReLU(alpha = 0.2))\n",
        "        model.add(MaxPool2D())\n",
        "        model.add(Conv2D(32, kernel_size=5, padding=\"same\"))\n",
        "        model.add(BatchNormalization()) #\n",
        "        model.add(LeakyReLU(alpha = 0.2))\n",
        "        model.add(MaxPool2D())\n",
        "        model.add(Conv2D(64, kernel_size=5, padding=\"same\"))\n",
        "        model.add(BatchNormalization()) #\n",
        "        model.add(LeakyReLU(alpha = 0.2))\n",
        "        model.add(MaxPool2D())\n",
        "        model.add(Conv2D(128, kernel_size=5, padding=\"same\"))\n",
        "        model.add(BatchNormalization()) #\n",
        "        model.add(LeakyReLU(alpha = 0.2))\n",
        "        model.add(MaxPool2D())\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(1 , activation = 'sigmoid'))\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "        validity = model(img)\n",
        "\n",
        "        return Model(img, validity)\n",
        "\n",
        "\n",
        "\n",
        "    def train(self, epochs, batch_size=128, save_interval=50):\n",
        "\n",
        "        # Load the dataset\n",
        "        dirname = '/content/drive/My Drive/DSET_TRIAL_100/3/'\n",
        "        X_train = []\n",
        "        for filename in os.listdir(dirname):\n",
        "          img = Image.open(dirname + filename)\n",
        "          img = np.asarray(img)\n",
        "          img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "          X_train.append(img)\n",
        "      \n",
        "        # Rescale -1 to 1\n",
        "        for x in X_train:\n",
        "          x = x / 127.5 - 1\n",
        " \n",
        "        X_train = np.expand_dims(X_train, axis=3)\n",
        "        print(np.shape(X_train))\n",
        "\n",
        "        X_train = np.asarray(X_train)\n",
        "        # Adversarial ground truths\n",
        "        \n",
        "        # valid = np.ones((batch_size, 1))\n",
        "        valid = []\n",
        "        valid = [0.9 for i in range(batch_size)]\n",
        "        valid = np.expand_dims(valid , axis=1)\n",
        "        print(valid.shape)\n",
        "        # fake = np.zeros((batch_size, 1))\n",
        "        fake = []\n",
        "        fake = [0.1 for i in range(batch_size)]\n",
        "        fake = np.expand_dims(fake , axis=1)\n",
        "        print(fake.shape)\n",
        "\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            # Select a random half of images\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            imgs = X_train[idx]\n",
        "\n",
        "            # Sample noise and generate a batch of new images\n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "            gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "            # Train the discriminator (real classified as ones and generated as zeros)\n",
        "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
        "            d_loss = np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            # Train the generator (wants discriminator to mistake images as real)\n",
        "            g_loss = self.combined.train_on_batch(noise, valid)\n",
        "\n",
        "            # Plot the progress\n",
        "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 50*d_loss[1], g_loss))\n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % save_interval == 0:\n",
        "                self.save_imgs(epoch)\n",
        "            if epoch % 20 == 0:\n",
        "                self.generator.save('/content/drive/My Drive/GEN_MODELS_1/gen_' + str(epoch) + '.h5')\n",
        "\n",
        "    def save_imgs(self, epoch):\n",
        "        noise = np.random.normal(0, 1, (5, self.latent_dim))\n",
        "        gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "        # Rescale images 0 - 1\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "\n",
        "        cnt = 0 \n",
        "        for img in gen_imgs:\n",
        "          img = np.squeeze(img , axis=2)\n",
        "          # print(np.shape(img))\n",
        "          plt.imsave(\"/content/drive/My Drive/GAN_IMGS/\" + str(epoch) + '_' + str(cnt) + \".png\" , img , cmap='gray')\n",
        "          cnt+=1\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    dcgan = DCGAN()\n",
        "    dcgan.train(epochs=2000, batch_size=8, save_interval=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_21 (Conv2D)           (None, 64, 64, 16)        416       \n",
            "_________________________________________________________________\n",
            "batch_normalization_36 (Batc (None, 64, 64, 16)        64        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_36 (LeakyReLU)   (None, 64, 64, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 32, 32, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 32, 32, 32)        12832     \n",
            "_________________________________________________________________\n",
            "batch_normalization_37 (Batc (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_37 (LeakyReLU)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 16, 16, 64)        51264     \n",
            "_________________________________________________________________\n",
            "batch_normalization_38 (Batc (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_38 (LeakyReLU)   (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 8, 8, 128)         204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization_39 (Batc (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_39 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 272,449\n",
            "Trainable params: 271,969\n",
            "Non-trainable params: 480\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_7 (Dense)              (None, 2048)              67584     \n",
            "_________________________________________________________________\n",
            "reshape_3 (Reshape)          (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_40 (Batc (None, 4, 4, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_40 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_12 (UpSampling (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_12 (Conv2DT (None, 8, 8, 64)          204864    \n",
            "_________________________________________________________________\n",
            "batch_normalization_41 (Batc (None, 8, 8, 64)          256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_41 (LeakyReLU)   (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_13 (UpSampling (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_13 (Conv2DT (None, 16, 16, 32)        51232     \n",
            "_________________________________________________________________\n",
            "batch_normalization_42 (Batc (None, 16, 16, 32)        128       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_42 (LeakyReLU)   (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_14 (UpSampling (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_14 (Conv2DT (None, 32, 32, 16)        12816     \n",
            "_________________________________________________________________\n",
            "batch_normalization_43 (Batc (None, 32, 32, 16)        64        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_43 (LeakyReLU)   (None, 32, 32, 16)        0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_15 (UpSampling (None, 64, 64, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_15 (Conv2DT (None, 64, 64, 8)         3208      \n",
            "_________________________________________________________________\n",
            "batch_normalization_44 (Batc (None, 64, 64, 8)         32        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_44 (LeakyReLU)   (None, 64, 64, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 64, 64, 8)         1608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_45 (Batc (None, 64, 64, 8)         32        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_45 (LeakyReLU)   (None, 64, 64, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 64, 64, 4)         804       \n",
            "_________________________________________________________________\n",
            "batch_normalization_46 (Batc (None, 64, 64, 4)         16        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_46 (LeakyReLU)   (None, 64, 64, 4)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 64, 64, 1)         101       \n",
            "_________________________________________________________________\n",
            "batch_normalization_47 (Batc (None, 64, 64, 1)         4         \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_47 (LeakyReLU)   (None, 64, 64, 1)         0         \n",
            "=================================================================\n",
            "Total params: 343,261\n",
            "Trainable params: 342,739\n",
            "Non-trainable params: 522\n",
            "_________________________________________________________________\n",
            "(100, 64, 64, 1)\n",
            "(8, 1)\n",
            "(8, 1)\n",
            "0 [D loss: 2.396456, acc.: 0.00%] [G loss: 0.688800]\n",
            "1 [D loss: 1.315687, acc.: 0.00%] [G loss: 0.690482]\n",
            "2 [D loss: 1.008744, acc.: 0.00%] [G loss: 0.687034]\n",
            "3 [D loss: 1.062075, acc.: 0.00%] [G loss: 0.687472]\n",
            "4 [D loss: 0.969325, acc.: 0.00%] [G loss: 0.688615]\n",
            "5 [D loss: 0.870635, acc.: 0.00%] [G loss: 0.686222]\n",
            "6 [D loss: 0.963161, acc.: 0.00%] [G loss: 0.686508]\n",
            "7 [D loss: 0.762790, acc.: 0.00%] [G loss: 0.686886]\n",
            "8 [D loss: 0.801818, acc.: 0.00%] [G loss: 0.689098]\n",
            "9 [D loss: 0.868632, acc.: 0.00%] [G loss: 0.689131]\n",
            "10 [D loss: 0.777854, acc.: 0.00%] [G loss: 0.684171]\n",
            "11 [D loss: 0.692432, acc.: 0.00%] [G loss: 0.683658]\n",
            "12 [D loss: 0.696544, acc.: 0.00%] [G loss: 0.680444]\n",
            "13 [D loss: 0.697401, acc.: 0.00%] [G loss: 0.682236]\n",
            "14 [D loss: 0.700436, acc.: 0.00%] [G loss: 0.677968]\n",
            "15 [D loss: 0.757895, acc.: 0.00%] [G loss: 0.682187]\n",
            "16 [D loss: 0.701434, acc.: 0.00%] [G loss: 0.681697]\n",
            "17 [D loss: 0.672585, acc.: 0.00%] [G loss: 0.679140]\n",
            "18 [D loss: 0.683077, acc.: 0.00%] [G loss: 0.676051]\n",
            "19 [D loss: 0.704350, acc.: 0.00%] [G loss: 0.679912]\n",
            "20 [D loss: 0.703387, acc.: 0.00%] [G loss: 0.682199]\n",
            "21 [D loss: 0.661178, acc.: 0.00%] [G loss: 0.684114]\n",
            "22 [D loss: 0.673693, acc.: 0.00%] [G loss: 0.681737]\n",
            "23 [D loss: 0.665768, acc.: 0.00%] [G loss: 0.681272]\n",
            "24 [D loss: 0.680314, acc.: 0.00%] [G loss: 0.682246]\n",
            "25 [D loss: 0.673067, acc.: 0.00%] [G loss: 0.677469]\n",
            "26 [D loss: 0.688384, acc.: 0.00%] [G loss: 0.663788]\n",
            "27 [D loss: 0.670701, acc.: 0.00%] [G loss: 0.664069]\n",
            "28 [D loss: 0.670617, acc.: 0.00%] [G loss: 0.664303]\n",
            "29 [D loss: 0.659953, acc.: 0.00%] [G loss: 0.666736]\n",
            "30 [D loss: 0.682100, acc.: 0.00%] [G loss: 0.668861]\n",
            "31 [D loss: 0.689109, acc.: 0.00%] [G loss: 0.664759]\n",
            "32 [D loss: 0.681620, acc.: 0.00%] [G loss: 0.662406]\n",
            "33 [D loss: 0.667399, acc.: 0.00%] [G loss: 0.656504]\n",
            "34 [D loss: 0.672026, acc.: 0.00%] [G loss: 0.662978]\n",
            "35 [D loss: 0.692238, acc.: 0.00%] [G loss: 0.651288]\n",
            "36 [D loss: 0.682304, acc.: 0.00%] [G loss: 0.653230]\n",
            "37 [D loss: 0.711728, acc.: 0.00%] [G loss: 0.658123]\n",
            "38 [D loss: 0.669645, acc.: 0.00%] [G loss: 0.660309]\n",
            "39 [D loss: 0.670021, acc.: 0.00%] [G loss: 0.665650]\n",
            "40 [D loss: 0.661741, acc.: 0.00%] [G loss: 0.666036]\n",
            "41 [D loss: 0.712936, acc.: 0.00%] [G loss: 0.670161]\n",
            "42 [D loss: 0.720591, acc.: 0.00%] [G loss: 0.664982]\n",
            "43 [D loss: 0.663677, acc.: 0.00%] [G loss: 0.654812]\n",
            "44 [D loss: 0.674351, acc.: 0.00%] [G loss: 0.651575]\n",
            "45 [D loss: 0.697628, acc.: 0.00%] [G loss: 0.657673]\n",
            "46 [D loss: 0.694067, acc.: 0.00%] [G loss: 0.652202]\n",
            "47 [D loss: 0.681862, acc.: 0.00%] [G loss: 0.625940]\n",
            "48 [D loss: 0.700053, acc.: 0.00%] [G loss: 0.628162]\n",
            "49 [D loss: 0.668557, acc.: 0.00%] [G loss: 0.629645]\n",
            "50 [D loss: 0.663800, acc.: 0.00%] [G loss: 0.625524]\n",
            "51 [D loss: 0.685412, acc.: 0.00%] [G loss: 0.617827]\n",
            "52 [D loss: 0.693781, acc.: 0.00%] [G loss: 0.633057]\n",
            "53 [D loss: 0.697213, acc.: 0.00%] [G loss: 0.629291]\n",
            "54 [D loss: 0.668635, acc.: 0.00%] [G loss: 0.613708]\n",
            "55 [D loss: 0.711613, acc.: 0.00%] [G loss: 0.626890]\n",
            "56 [D loss: 0.672938, acc.: 0.00%] [G loss: 0.625964]\n",
            "57 [D loss: 0.686160, acc.: 0.00%] [G loss: 0.623817]\n",
            "58 [D loss: 0.671758, acc.: 0.00%] [G loss: 0.608439]\n",
            "59 [D loss: 0.684082, acc.: 0.00%] [G loss: 0.624803]\n",
            "60 [D loss: 0.698467, acc.: 0.00%] [G loss: 0.610242]\n",
            "61 [D loss: 0.712708, acc.: 0.00%] [G loss: 0.623029]\n",
            "62 [D loss: 0.699819, acc.: 0.00%] [G loss: 0.614505]\n",
            "63 [D loss: 0.671959, acc.: 0.00%] [G loss: 0.597527]\n",
            "64 [D loss: 0.664114, acc.: 0.00%] [G loss: 0.588036]\n",
            "65 [D loss: 0.678241, acc.: 0.00%] [G loss: 0.591690]\n",
            "66 [D loss: 0.664122, acc.: 0.00%] [G loss: 0.578899]\n",
            "67 [D loss: 0.659379, acc.: 0.00%] [G loss: 0.577891]\n",
            "68 [D loss: 0.681124, acc.: 0.00%] [G loss: 0.581262]\n",
            "69 [D loss: 0.662479, acc.: 0.00%] [G loss: 0.579573]\n",
            "70 [D loss: 0.667513, acc.: 0.00%] [G loss: 0.581594]\n",
            "71 [D loss: 0.675963, acc.: 0.00%] [G loss: 0.582891]\n",
            "72 [D loss: 0.671296, acc.: 0.00%] [G loss: 0.577873]\n",
            "73 [D loss: 0.681452, acc.: 0.00%] [G loss: 0.579181]\n",
            "74 [D loss: 0.677797, acc.: 0.00%] [G loss: 0.597570]\n",
            "75 [D loss: 0.682194, acc.: 0.00%] [G loss: 0.588014]\n",
            "76 [D loss: 0.669931, acc.: 0.00%] [G loss: 0.577434]\n",
            "77 [D loss: 0.678819, acc.: 0.00%] [G loss: 0.577745]\n",
            "78 [D loss: 0.687379, acc.: 0.00%] [G loss: 0.586755]\n",
            "79 [D loss: 0.695678, acc.: 0.00%] [G loss: 0.571559]\n",
            "80 [D loss: 0.668146, acc.: 0.00%] [G loss: 0.558082]\n",
            "81 [D loss: 0.679355, acc.: 0.00%] [G loss: 0.568391]\n",
            "82 [D loss: 0.659196, acc.: 0.00%] [G loss: 0.568307]\n",
            "83 [D loss: 0.665160, acc.: 0.00%] [G loss: 0.566419]\n",
            "84 [D loss: 0.661522, acc.: 0.00%] [G loss: 0.570814]\n",
            "85 [D loss: 0.665104, acc.: 0.00%] [G loss: 0.553994]\n",
            "86 [D loss: 0.681566, acc.: 0.00%] [G loss: 0.555650]\n",
            "87 [D loss: 0.668655, acc.: 0.00%] [G loss: 0.566109]\n",
            "88 [D loss: 0.660986, acc.: 0.00%] [G loss: 0.561628]\n",
            "89 [D loss: 0.658144, acc.: 0.00%] [G loss: 0.555324]\n",
            "90 [D loss: 0.661530, acc.: 0.00%] [G loss: 0.560596]\n",
            "91 [D loss: 0.671048, acc.: 0.00%] [G loss: 0.564570]\n",
            "92 [D loss: 0.670059, acc.: 0.00%] [G loss: 0.564222]\n",
            "93 [D loss: 0.677766, acc.: 0.00%] [G loss: 0.559599]\n",
            "94 [D loss: 0.664783, acc.: 0.00%] [G loss: 0.557500]\n",
            "95 [D loss: 0.677493, acc.: 0.00%] [G loss: 0.548171]\n",
            "96 [D loss: 0.682791, acc.: 0.00%] [G loss: 0.559096]\n",
            "97 [D loss: 0.678704, acc.: 0.00%] [G loss: 0.538867]\n",
            "98 [D loss: 0.666909, acc.: 0.00%] [G loss: 0.536101]\n",
            "99 [D loss: 0.663571, acc.: 0.00%] [G loss: 0.551031]\n",
            "100 [D loss: 0.662496, acc.: 0.00%] [G loss: 0.569228]\n",
            "101 [D loss: 0.675169, acc.: 0.00%] [G loss: 0.549646]\n",
            "102 [D loss: 0.666204, acc.: 0.00%] [G loss: 0.541279]\n",
            "103 [D loss: 0.668802, acc.: 0.00%] [G loss: 0.565907]\n",
            "104 [D loss: 0.674576, acc.: 0.00%] [G loss: 0.552473]\n",
            "105 [D loss: 0.665810, acc.: 0.00%] [G loss: 0.536784]\n",
            "106 [D loss: 0.691571, acc.: 0.00%] [G loss: 0.572111]\n",
            "107 [D loss: 0.707191, acc.: 0.00%] [G loss: 0.544866]\n",
            "108 [D loss: 0.660542, acc.: 0.00%] [G loss: 0.525015]\n",
            "109 [D loss: 0.686103, acc.: 0.00%] [G loss: 0.543387]\n",
            "110 [D loss: 0.667227, acc.: 0.00%] [G loss: 0.535261]\n",
            "111 [D loss: 0.705958, acc.: 0.00%] [G loss: 0.545342]\n",
            "112 [D loss: 0.668926, acc.: 0.00%] [G loss: 0.543877]\n",
            "113 [D loss: 0.659406, acc.: 0.00%] [G loss: 0.545195]\n",
            "114 [D loss: 0.662332, acc.: 0.00%] [G loss: 0.550509]\n",
            "115 [D loss: 0.666755, acc.: 0.00%] [G loss: 0.546641]\n",
            "116 [D loss: 0.662134, acc.: 0.00%] [G loss: 0.558542]\n",
            "117 [D loss: 0.660642, acc.: 0.00%] [G loss: 0.556046]\n",
            "118 [D loss: 0.659269, acc.: 0.00%] [G loss: 0.560771]\n",
            "119 [D loss: 0.664912, acc.: 0.00%] [G loss: 0.546471]\n",
            "120 [D loss: 0.674420, acc.: 0.00%] [G loss: 0.575347]\n",
            "121 [D loss: 0.673318, acc.: 0.00%] [G loss: 0.571643]\n",
            "122 [D loss: 0.666228, acc.: 0.00%] [G loss: 0.576537]\n",
            "123 [D loss: 0.674272, acc.: 0.00%] [G loss: 0.561365]\n",
            "124 [D loss: 0.666448, acc.: 0.00%] [G loss: 0.564375]\n",
            "125 [D loss: 0.661032, acc.: 0.00%] [G loss: 0.563035]\n",
            "126 [D loss: 0.659218, acc.: 0.00%] [G loss: 0.560367]\n",
            "127 [D loss: 0.666431, acc.: 0.00%] [G loss: 0.554923]\n",
            "128 [D loss: 0.658924, acc.: 0.00%] [G loss: 0.554944]\n",
            "129 [D loss: 0.671159, acc.: 0.00%] [G loss: 0.548006]\n",
            "130 [D loss: 0.679555, acc.: 0.00%] [G loss: 0.585693]\n",
            "131 [D loss: 0.704178, acc.: 0.00%] [G loss: 0.554725]\n",
            "132 [D loss: 0.671628, acc.: 0.00%] [G loss: 0.564765]\n",
            "133 [D loss: 0.672557, acc.: 0.00%] [G loss: 0.590612]\n",
            "134 [D loss: 0.673869, acc.: 0.00%] [G loss: 0.588394]\n",
            "135 [D loss: 0.661381, acc.: 0.00%] [G loss: 0.576602]\n",
            "136 [D loss: 0.656067, acc.: 0.00%] [G loss: 0.585483]\n",
            "137 [D loss: 0.680366, acc.: 0.00%] [G loss: 0.597974]\n",
            "138 [D loss: 0.668301, acc.: 0.00%] [G loss: 0.598144]\n",
            "139 [D loss: 0.674174, acc.: 0.00%] [G loss: 0.573843]\n",
            "140 [D loss: 0.676562, acc.: 0.00%] [G loss: 0.596785]\n",
            "141 [D loss: 0.664609, acc.: 0.00%] [G loss: 0.605207]\n",
            "142 [D loss: 0.684341, acc.: 0.00%] [G loss: 0.624601]\n",
            "143 [D loss: 0.669854, acc.: 0.00%] [G loss: 0.596760]\n",
            "144 [D loss: 0.663967, acc.: 0.00%] [G loss: 0.575521]\n",
            "145 [D loss: 0.677330, acc.: 0.00%] [G loss: 0.600658]\n",
            "146 [D loss: 0.708099, acc.: 0.00%] [G loss: 0.582936]\n",
            "147 [D loss: 0.694624, acc.: 0.00%] [G loss: 0.621130]\n",
            "148 [D loss: 0.681939, acc.: 0.00%] [G loss: 0.582412]\n",
            "149 [D loss: 0.688665, acc.: 0.00%] [G loss: 0.621623]\n",
            "150 [D loss: 0.695886, acc.: 0.00%] [G loss: 0.584919]\n",
            "151 [D loss: 0.679336, acc.: 0.00%] [G loss: 0.589178]\n",
            "152 [D loss: 0.660811, acc.: 0.00%] [G loss: 0.601997]\n",
            "153 [D loss: 0.659866, acc.: 0.00%] [G loss: 0.606135]\n",
            "154 [D loss: 0.669638, acc.: 0.00%] [G loss: 0.598553]\n",
            "155 [D loss: 0.665875, acc.: 0.00%] [G loss: 0.635017]\n",
            "156 [D loss: 0.671742, acc.: 0.00%] [G loss: 0.644030]\n",
            "157 [D loss: 0.674042, acc.: 0.00%] [G loss: 0.647910]\n",
            "158 [D loss: 0.677990, acc.: 0.00%] [G loss: 0.622970]\n",
            "159 [D loss: 0.667336, acc.: 0.00%] [G loss: 0.610868]\n",
            "160 [D loss: 0.666485, acc.: 0.00%] [G loss: 0.612908]\n",
            "161 [D loss: 0.662127, acc.: 0.00%] [G loss: 0.600883]\n",
            "162 [D loss: 0.666335, acc.: 0.00%] [G loss: 0.572705]\n",
            "163 [D loss: 0.671145, acc.: 0.00%] [G loss: 0.574758]\n",
            "164 [D loss: 0.679073, acc.: 0.00%] [G loss: 0.566955]\n",
            "165 [D loss: 0.699347, acc.: 0.00%] [G loss: 0.613509]\n",
            "166 [D loss: 0.707603, acc.: 0.00%] [G loss: 0.585787]\n",
            "167 [D loss: 0.666504, acc.: 0.00%] [G loss: 0.589837]\n",
            "168 [D loss: 0.670672, acc.: 0.00%] [G loss: 0.590992]\n",
            "169 [D loss: 0.661169, acc.: 0.00%] [G loss: 0.600871]\n",
            "170 [D loss: 0.664856, acc.: 0.00%] [G loss: 0.597367]\n",
            "171 [D loss: 0.661635, acc.: 0.00%] [G loss: 0.583437]\n",
            "172 [D loss: 0.668982, acc.: 0.00%] [G loss: 0.595507]\n",
            "173 [D loss: 0.669990, acc.: 0.00%] [G loss: 0.592450]\n",
            "174 [D loss: 0.656291, acc.: 0.00%] [G loss: 0.580222]\n",
            "175 [D loss: 0.664200, acc.: 0.00%] [G loss: 0.580945]\n",
            "176 [D loss: 0.659141, acc.: 0.00%] [G loss: 0.593757]\n",
            "177 [D loss: 0.674853, acc.: 0.00%] [G loss: 0.555396]\n",
            "178 [D loss: 0.664979, acc.: 0.00%] [G loss: 0.568893]\n",
            "179 [D loss: 0.659929, acc.: 0.00%] [G loss: 0.592104]\n",
            "180 [D loss: 0.661477, acc.: 0.00%] [G loss: 0.576813]\n",
            "181 [D loss: 0.656007, acc.: 0.00%] [G loss: 0.592445]\n",
            "182 [D loss: 0.663191, acc.: 0.00%] [G loss: 0.576643]\n",
            "183 [D loss: 0.663550, acc.: 0.00%] [G loss: 0.595340]\n",
            "184 [D loss: 0.658225, acc.: 0.00%] [G loss: 0.590147]\n",
            "185 [D loss: 0.665612, acc.: 0.00%] [G loss: 0.589530]\n",
            "186 [D loss: 0.663596, acc.: 0.00%] [G loss: 0.576874]\n",
            "187 [D loss: 0.662546, acc.: 0.00%] [G loss: 0.588138]\n",
            "188 [D loss: 0.664683, acc.: 0.00%] [G loss: 0.578775]\n",
            "189 [D loss: 0.681748, acc.: 0.00%] [G loss: 0.605161]\n",
            "190 [D loss: 0.696379, acc.: 0.00%] [G loss: 0.591126]\n",
            "191 [D loss: 0.671120, acc.: 0.00%] [G loss: 0.625069]\n",
            "192 [D loss: 0.663603, acc.: 0.00%] [G loss: 0.649260]\n",
            "193 [D loss: 0.663127, acc.: 0.00%] [G loss: 0.616457]\n",
            "194 [D loss: 0.664971, acc.: 0.00%] [G loss: 0.626278]\n",
            "195 [D loss: 0.661474, acc.: 0.00%] [G loss: 0.613259]\n",
            "196 [D loss: 0.690962, acc.: 0.00%] [G loss: 0.645794]\n",
            "197 [D loss: 0.695151, acc.: 0.00%] [G loss: 0.616278]\n",
            "198 [D loss: 0.674659, acc.: 0.00%] [G loss: 0.650963]\n",
            "199 [D loss: 0.688128, acc.: 0.00%] [G loss: 0.627635]\n",
            "200 [D loss: 0.675127, acc.: 0.00%] [G loss: 0.586414]\n",
            "201 [D loss: 0.671803, acc.: 0.00%] [G loss: 0.597681]\n",
            "202 [D loss: 0.686258, acc.: 0.00%] [G loss: 0.657678]\n",
            "203 [D loss: 0.708384, acc.: 0.00%] [G loss: 0.642508]\n",
            "204 [D loss: 0.666592, acc.: 0.00%] [G loss: 0.635423]\n",
            "205 [D loss: 0.657558, acc.: 0.00%] [G loss: 0.625334]\n",
            "206 [D loss: 0.658298, acc.: 0.00%] [G loss: 0.612096]\n",
            "207 [D loss: 0.656766, acc.: 0.00%] [G loss: 0.617535]\n",
            "208 [D loss: 0.671600, acc.: 0.00%] [G loss: 0.657370]\n",
            "209 [D loss: 0.697562, acc.: 0.00%] [G loss: 0.621857]\n",
            "210 [D loss: 0.677024, acc.: 0.00%] [G loss: 0.648503]\n",
            "211 [D loss: 0.711889, acc.: 0.00%] [G loss: 0.650257]\n",
            "212 [D loss: 0.659572, acc.: 0.00%] [G loss: 0.645894]\n",
            "213 [D loss: 0.661960, acc.: 0.00%] [G loss: 0.635157]\n",
            "214 [D loss: 0.669764, acc.: 0.00%] [G loss: 0.625789]\n",
            "215 [D loss: 0.667613, acc.: 0.00%] [G loss: 0.610352]\n",
            "216 [D loss: 0.656651, acc.: 0.00%] [G loss: 0.612826]\n",
            "217 [D loss: 0.662042, acc.: 0.00%] [G loss: 0.633831]\n",
            "218 [D loss: 0.655549, acc.: 0.00%] [G loss: 0.621645]\n",
            "219 [D loss: 0.680297, acc.: 0.00%] [G loss: 0.694920]\n",
            "220 [D loss: 0.704140, acc.: 0.00%] [G loss: 0.655200]\n",
            "221 [D loss: 0.662003, acc.: 0.00%] [G loss: 0.660498]\n",
            "222 [D loss: 0.662154, acc.: 0.00%] [G loss: 0.708651]\n",
            "223 [D loss: 0.656723, acc.: 0.00%] [G loss: 0.679904]\n",
            "224 [D loss: 0.662693, acc.: 0.00%] [G loss: 0.700205]\n",
            "225 [D loss: 0.666623, acc.: 0.00%] [G loss: 0.691959]\n",
            "226 [D loss: 0.656083, acc.: 0.00%] [G loss: 0.657966]\n",
            "227 [D loss: 0.680837, acc.: 0.00%] [G loss: 0.701978]\n",
            "228 [D loss: 0.726567, acc.: 0.00%] [G loss: 0.685047]\n",
            "229 [D loss: 0.667985, acc.: 0.00%] [G loss: 0.673666]\n",
            "230 [D loss: 0.664746, acc.: 0.00%] [G loss: 0.689916]\n",
            "231 [D loss: 0.662228, acc.: 0.00%] [G loss: 0.684599]\n",
            "232 [D loss: 0.662037, acc.: 0.00%] [G loss: 0.640167]\n",
            "233 [D loss: 0.664029, acc.: 0.00%] [G loss: 0.647158]\n",
            "234 [D loss: 0.655779, acc.: 0.00%] [G loss: 0.661885]\n",
            "235 [D loss: 0.657990, acc.: 0.00%] [G loss: 0.660786]\n",
            "236 [D loss: 0.663344, acc.: 0.00%] [G loss: 0.668598]\n",
            "237 [D loss: 0.669820, acc.: 0.00%] [G loss: 0.631260]\n",
            "238 [D loss: 0.661979, acc.: 0.00%] [G loss: 0.649963]\n",
            "239 [D loss: 0.655434, acc.: 0.00%] [G loss: 0.660685]\n",
            "240 [D loss: 0.657400, acc.: 0.00%] [G loss: 0.652786]\n",
            "241 [D loss: 0.657143, acc.: 0.00%] [G loss: 0.660045]\n",
            "242 [D loss: 0.657034, acc.: 0.00%] [G loss: 0.669621]\n",
            "243 [D loss: 0.658765, acc.: 0.00%] [G loss: 0.673089]\n",
            "244 [D loss: 0.663895, acc.: 0.00%] [G loss: 0.688100]\n",
            "245 [D loss: 0.665446, acc.: 0.00%] [G loss: 0.657854]\n",
            "246 [D loss: 0.653246, acc.: 0.00%] [G loss: 0.667091]\n",
            "247 [D loss: 0.657245, acc.: 0.00%] [G loss: 0.698260]\n",
            "248 [D loss: 0.673869, acc.: 0.00%] [G loss: 0.631760]\n",
            "249 [D loss: 0.666187, acc.: 0.00%] [G loss: 0.663779]\n",
            "250 [D loss: 0.658794, acc.: 0.00%] [G loss: 0.654607]\n",
            "251 [D loss: 0.652974, acc.: 0.00%] [G loss: 0.663611]\n",
            "252 [D loss: 0.655541, acc.: 0.00%] [G loss: 0.666237]\n",
            "253 [D loss: 0.664672, acc.: 0.00%] [G loss: 0.650507]\n",
            "254 [D loss: 0.665347, acc.: 0.00%] [G loss: 0.674139]\n",
            "255 [D loss: 0.675874, acc.: 0.00%] [G loss: 0.623865]\n",
            "256 [D loss: 0.698168, acc.: 0.00%] [G loss: 0.712282]\n",
            "257 [D loss: 0.750572, acc.: 0.00%] [G loss: 0.672629]\n",
            "258 [D loss: 0.659419, acc.: 0.00%] [G loss: 0.663032]\n",
            "259 [D loss: 0.664431, acc.: 0.00%] [G loss: 0.675580]\n",
            "260 [D loss: 0.671080, acc.: 0.00%] [G loss: 0.638979]\n",
            "261 [D loss: 0.682506, acc.: 0.00%] [G loss: 0.708944]\n",
            "262 [D loss: 0.717033, acc.: 0.00%] [G loss: 0.685148]\n",
            "263 [D loss: 0.666500, acc.: 0.00%] [G loss: 0.662034]\n",
            "264 [D loss: 0.671003, acc.: 0.00%] [G loss: 0.668942]\n",
            "265 [D loss: 0.664765, acc.: 0.00%] [G loss: 0.695389]\n",
            "266 [D loss: 0.664309, acc.: 0.00%] [G loss: 0.681074]\n",
            "267 [D loss: 0.668283, acc.: 0.00%] [G loss: 0.698495]\n",
            "268 [D loss: 0.664681, acc.: 0.00%] [G loss: 0.693486]\n",
            "269 [D loss: 0.663477, acc.: 0.00%] [G loss: 0.697265]\n",
            "270 [D loss: 0.665034, acc.: 0.00%] [G loss: 0.702427]\n",
            "271 [D loss: 0.656189, acc.: 0.00%] [G loss: 0.705353]\n",
            "272 [D loss: 0.654505, acc.: 0.00%] [G loss: 0.713828]\n",
            "273 [D loss: 0.656534, acc.: 0.00%] [G loss: 0.729848]\n",
            "274 [D loss: 0.671067, acc.: 0.00%] [G loss: 0.698696]\n",
            "275 [D loss: 0.678646, acc.: 0.00%] [G loss: 0.762942]\n",
            "276 [D loss: 0.726051, acc.: 0.00%] [G loss: 0.710469]\n",
            "277 [D loss: 0.665186, acc.: 0.00%] [G loss: 0.718890]\n",
            "278 [D loss: 0.671434, acc.: 0.00%] [G loss: 0.690662]\n",
            "279 [D loss: 0.687622, acc.: 0.00%] [G loss: 0.726636]\n",
            "280 [D loss: 0.697737, acc.: 0.00%] [G loss: 0.713408]\n",
            "281 [D loss: 0.660292, acc.: 0.00%] [G loss: 0.718453]\n",
            "282 [D loss: 0.663870, acc.: 0.00%] [G loss: 0.725960]\n",
            "283 [D loss: 0.663384, acc.: 0.00%] [G loss: 0.687339]\n",
            "284 [D loss: 0.655497, acc.: 0.00%] [G loss: 0.691246]\n",
            "285 [D loss: 0.655902, acc.: 0.00%] [G loss: 0.702689]\n",
            "286 [D loss: 0.665858, acc.: 0.00%] [G loss: 0.738775]\n",
            "287 [D loss: 0.679674, acc.: 0.00%] [G loss: 0.718491]\n",
            "288 [D loss: 0.661606, acc.: 0.00%] [G loss: 0.728234]\n",
            "289 [D loss: 0.664643, acc.: 0.00%] [G loss: 0.709305]\n",
            "290 [D loss: 0.664016, acc.: 0.00%] [G loss: 0.691781]\n",
            "291 [D loss: 0.667553, acc.: 0.00%] [G loss: 0.680700]\n",
            "292 [D loss: 0.658119, acc.: 0.00%] [G loss: 0.692048]\n",
            "293 [D loss: 0.656542, acc.: 0.00%] [G loss: 0.698845]\n",
            "294 [D loss: 0.658376, acc.: 0.00%] [G loss: 0.701950]\n",
            "295 [D loss: 0.657874, acc.: 0.00%] [G loss: 0.709944]\n",
            "296 [D loss: 0.654422, acc.: 0.00%] [G loss: 0.713299]\n",
            "297 [D loss: 0.654228, acc.: 0.00%] [G loss: 0.713571]\n",
            "298 [D loss: 0.657501, acc.: 0.00%] [G loss: 0.698442]\n",
            "299 [D loss: 0.660601, acc.: 0.00%] [G loss: 0.728376]\n",
            "300 [D loss: 0.672897, acc.: 0.00%] [G loss: 0.689362]\n",
            "301 [D loss: 0.687975, acc.: 0.00%] [G loss: 0.741351]\n",
            "302 [D loss: 0.705545, acc.: 0.00%] [G loss: 0.709333]\n",
            "303 [D loss: 0.668529, acc.: 0.00%] [G loss: 0.722431]\n",
            "304 [D loss: 0.686987, acc.: 0.00%] [G loss: 0.693016]\n",
            "305 [D loss: 0.666153, acc.: 0.00%] [G loss: 0.721980]\n",
            "306 [D loss: 0.659982, acc.: 0.00%] [G loss: 0.690069]\n",
            "307 [D loss: 0.657907, acc.: 0.00%] [G loss: 0.710371]\n",
            "308 [D loss: 0.656802, acc.: 0.00%] [G loss: 0.695194]\n",
            "309 [D loss: 0.656632, acc.: 0.00%] [G loss: 0.686895]\n",
            "310 [D loss: 0.659325, acc.: 0.00%] [G loss: 0.714939]\n",
            "311 [D loss: 0.679505, acc.: 0.00%] [G loss: 0.666548]\n",
            "312 [D loss: 0.690371, acc.: 0.00%] [G loss: 0.746278]\n",
            "313 [D loss: 0.749133, acc.: 0.00%] [G loss: 0.738198]\n",
            "314 [D loss: 0.659581, acc.: 0.00%] [G loss: 0.729789]\n",
            "315 [D loss: 0.670792, acc.: 0.00%] [G loss: 0.770993]\n",
            "316 [D loss: 0.691832, acc.: 0.00%] [G loss: 0.733233]\n",
            "317 [D loss: 0.665276, acc.: 0.00%] [G loss: 0.745929]\n",
            "318 [D loss: 0.658065, acc.: 0.00%] [G loss: 0.760746]\n",
            "319 [D loss: 0.660232, acc.: 0.00%] [G loss: 0.710662]\n",
            "320 [D loss: 0.662261, acc.: 0.00%] [G loss: 0.731713]\n",
            "321 [D loss: 0.675483, acc.: 0.00%] [G loss: 0.709779]\n",
            "322 [D loss: 0.663273, acc.: 0.00%] [G loss: 0.726113]\n",
            "323 [D loss: 0.675746, acc.: 0.00%] [G loss: 0.673254]\n",
            "324 [D loss: 0.685534, acc.: 0.00%] [G loss: 0.737116]\n",
            "325 [D loss: 0.678184, acc.: 0.00%] [G loss: 0.757966]\n",
            "326 [D loss: 0.688034, acc.: 0.00%] [G loss: 0.801208]\n",
            "327 [D loss: 0.733421, acc.: 0.00%] [G loss: 0.738380]\n",
            "328 [D loss: 0.664433, acc.: 0.00%] [G loss: 0.741937]\n",
            "329 [D loss: 0.668770, acc.: 0.00%] [G loss: 0.752081]\n",
            "330 [D loss: 0.677407, acc.: 0.00%] [G loss: 0.743443]\n",
            "331 [D loss: 0.693956, acc.: 0.00%] [G loss: 0.725556]\n",
            "332 [D loss: 0.679616, acc.: 0.00%] [G loss: 0.773461]\n",
            "333 [D loss: 0.693562, acc.: 0.00%] [G loss: 0.752407]\n",
            "334 [D loss: 0.685000, acc.: 0.00%] [G loss: 0.731731]\n",
            "335 [D loss: 0.720802, acc.: 0.00%] [G loss: 0.714105]\n",
            "336 [D loss: 0.665299, acc.: 0.00%] [G loss: 0.698955]\n",
            "337 [D loss: 0.665917, acc.: 0.00%] [G loss: 0.690127]\n",
            "338 [D loss: 0.662603, acc.: 0.00%] [G loss: 0.692307]\n",
            "339 [D loss: 0.670314, acc.: 0.00%] [G loss: 0.684672]\n",
            "340 [D loss: 0.662356, acc.: 0.00%] [G loss: 0.700727]\n",
            "341 [D loss: 0.667759, acc.: 0.00%] [G loss: 0.697549]\n",
            "342 [D loss: 0.662335, acc.: 0.00%] [G loss: 0.726638]\n",
            "343 [D loss: 0.673491, acc.: 0.00%] [G loss: 0.698301]\n",
            "344 [D loss: 0.677752, acc.: 0.00%] [G loss: 0.751639]\n",
            "345 [D loss: 0.703934, acc.: 0.00%] [G loss: 0.713211]\n",
            "346 [D loss: 0.667305, acc.: 0.00%] [G loss: 0.731501]\n",
            "347 [D loss: 0.674253, acc.: 0.00%] [G loss: 0.715276]\n",
            "348 [D loss: 0.658601, acc.: 0.00%] [G loss: 0.728550]\n",
            "349 [D loss: 0.655825, acc.: 0.00%] [G loss: 0.731714]\n",
            "350 [D loss: 0.658516, acc.: 0.00%] [G loss: 0.739903]\n",
            "351 [D loss: 0.664645, acc.: 0.00%] [G loss: 0.705496]\n",
            "352 [D loss: 0.658917, acc.: 0.00%] [G loss: 0.718101]\n",
            "353 [D loss: 0.661778, acc.: 0.00%] [G loss: 0.705703]\n",
            "354 [D loss: 0.661599, acc.: 0.00%] [G loss: 0.731218]\n",
            "355 [D loss: 0.660982, acc.: 0.00%] [G loss: 0.720434]\n",
            "356 [D loss: 0.691631, acc.: 0.00%] [G loss: 0.746270]\n",
            "357 [D loss: 0.741936, acc.: 0.00%] [G loss: 0.725798]\n",
            "358 [D loss: 0.657914, acc.: 0.00%] [G loss: 0.707107]\n",
            "359 [D loss: 0.657297, acc.: 0.00%] [G loss: 0.744086]\n",
            "360 [D loss: 0.665909, acc.: 0.00%] [G loss: 0.683923]\n",
            "361 [D loss: 0.662847, acc.: 0.00%] [G loss: 0.715174]\n",
            "362 [D loss: 0.663060, acc.: 0.00%] [G loss: 0.713738]\n",
            "363 [D loss: 0.668061, acc.: 0.00%] [G loss: 0.715340]\n",
            "364 [D loss: 0.674912, acc.: 0.00%] [G loss: 0.708105]\n",
            "365 [D loss: 0.662334, acc.: 0.00%] [G loss: 0.742965]\n",
            "366 [D loss: 0.664852, acc.: 0.00%] [G loss: 0.704567]\n",
            "367 [D loss: 0.673870, acc.: 0.00%] [G loss: 0.743963]\n",
            "368 [D loss: 0.728870, acc.: 0.00%] [G loss: 0.736456]\n",
            "369 [D loss: 0.666290, acc.: 0.00%] [G loss: 0.740195]\n",
            "370 [D loss: 0.671688, acc.: 0.00%] [G loss: 0.725152]\n",
            "371 [D loss: 0.661725, acc.: 0.00%] [G loss: 0.712512]\n",
            "372 [D loss: 0.668465, acc.: 0.00%] [G loss: 0.698136]\n",
            "373 [D loss: 0.662143, acc.: 0.00%] [G loss: 0.709148]\n",
            "374 [D loss: 0.671322, acc.: 0.00%] [G loss: 0.696819]\n",
            "375 [D loss: 0.661186, acc.: 0.00%] [G loss: 0.719146]\n",
            "376 [D loss: 0.667607, acc.: 0.00%] [G loss: 0.703241]\n",
            "377 [D loss: 0.661834, acc.: 0.00%] [G loss: 0.724380]\n",
            "378 [D loss: 0.667634, acc.: 0.00%] [G loss: 0.710740]\n",
            "379 [D loss: 0.657508, acc.: 0.00%] [G loss: 0.716864]\n",
            "380 [D loss: 0.660170, acc.: 0.00%] [G loss: 0.715890]\n",
            "381 [D loss: 0.654952, acc.: 0.00%] [G loss: 0.719792]\n",
            "382 [D loss: 0.653977, acc.: 0.00%] [G loss: 0.702324]\n",
            "383 [D loss: 0.653675, acc.: 0.00%] [G loss: 0.708349]\n",
            "384 [D loss: 0.652262, acc.: 0.00%] [G loss: 0.719326]\n",
            "385 [D loss: 0.658674, acc.: 0.00%] [G loss: 0.714373]\n",
            "386 [D loss: 0.653921, acc.: 0.00%] [G loss: 0.693155]\n",
            "387 [D loss: 0.655846, acc.: 0.00%] [G loss: 0.724540]\n",
            "388 [D loss: 0.653425, acc.: 0.00%] [G loss: 0.711910]\n",
            "389 [D loss: 0.655723, acc.: 0.00%] [G loss: 0.733409]\n",
            "390 [D loss: 0.661580, acc.: 0.00%] [G loss: 0.729475]\n",
            "391 [D loss: 0.665274, acc.: 0.00%] [G loss: 0.759978]\n",
            "392 [D loss: 0.678620, acc.: 0.00%] [G loss: 0.721851]\n",
            "393 [D loss: 0.665774, acc.: 0.00%] [G loss: 0.759926]\n",
            "394 [D loss: 0.668982, acc.: 0.00%] [G loss: 0.737577]\n",
            "395 [D loss: 0.670192, acc.: 0.00%] [G loss: 0.765465]\n",
            "396 [D loss: 0.693238, acc.: 0.00%] [G loss: 0.737496]\n",
            "397 [D loss: 0.661830, acc.: 0.00%] [G loss: 0.756362]\n",
            "398 [D loss: 0.669308, acc.: 0.00%] [G loss: 0.738348]\n",
            "399 [D loss: 0.661969, acc.: 0.00%] [G loss: 0.753184]\n",
            "400 [D loss: 0.658797, acc.: 0.00%] [G loss: 0.749984]\n",
            "401 [D loss: 0.651745, acc.: 0.00%] [G loss: 0.741784]\n",
            "402 [D loss: 0.652944, acc.: 0.00%] [G loss: 0.761509]\n",
            "403 [D loss: 0.654149, acc.: 0.00%] [G loss: 0.745529]\n",
            "404 [D loss: 0.652541, acc.: 0.00%] [G loss: 0.741758]\n",
            "405 [D loss: 0.656132, acc.: 0.00%] [G loss: 0.739245]\n",
            "406 [D loss: 0.654761, acc.: 0.00%] [G loss: 0.729565]\n",
            "407 [D loss: 0.655827, acc.: 0.00%] [G loss: 0.750136]\n",
            "408 [D loss: 0.665536, acc.: 0.00%] [G loss: 0.745303]\n",
            "409 [D loss: 0.661834, acc.: 0.00%] [G loss: 0.777427]\n",
            "410 [D loss: 0.668833, acc.: 0.00%] [G loss: 0.753411]\n",
            "411 [D loss: 0.659447, acc.: 0.00%] [G loss: 0.783516]\n",
            "412 [D loss: 0.668154, acc.: 0.00%] [G loss: 0.786021]\n",
            "413 [D loss: 0.658180, acc.: 0.00%] [G loss: 0.797025]\n",
            "414 [D loss: 0.660955, acc.: 0.00%] [G loss: 0.776284]\n",
            "415 [D loss: 0.665236, acc.: 0.00%] [G loss: 0.789969]\n",
            "416 [D loss: 0.674093, acc.: 0.00%] [G loss: 0.762813]\n",
            "417 [D loss: 0.657046, acc.: 0.00%] [G loss: 0.779161]\n",
            "418 [D loss: 0.653596, acc.: 0.00%] [G loss: 0.773536]\n",
            "419 [D loss: 0.654070, acc.: 0.00%] [G loss: 0.779631]\n",
            "420 [D loss: 0.657311, acc.: 0.00%] [G loss: 0.764635]\n",
            "421 [D loss: 0.658120, acc.: 0.00%] [G loss: 0.786702]\n",
            "422 [D loss: 0.662113, acc.: 0.00%] [G loss: 0.782928]\n",
            "423 [D loss: 0.656568, acc.: 0.00%] [G loss: 0.780562]\n",
            "424 [D loss: 0.656750, acc.: 0.00%] [G loss: 0.763190]\n",
            "425 [D loss: 0.653473, acc.: 0.00%] [G loss: 0.778723]\n",
            "426 [D loss: 0.651520, acc.: 0.00%] [G loss: 0.766234]\n",
            "427 [D loss: 0.653512, acc.: 0.00%] [G loss: 0.767095]\n",
            "428 [D loss: 0.652687, acc.: 0.00%] [G loss: 0.762286]\n",
            "429 [D loss: 0.655335, acc.: 0.00%] [G loss: 0.806674]\n",
            "430 [D loss: 0.652115, acc.: 0.00%] [G loss: 0.805159]\n",
            "431 [D loss: 0.653260, acc.: 0.00%] [G loss: 0.808965]\n",
            "432 [D loss: 0.658741, acc.: 0.00%] [G loss: 0.805140]\n",
            "433 [D loss: 0.661773, acc.: 0.00%] [G loss: 0.821087]\n",
            "434 [D loss: 0.668144, acc.: 0.00%] [G loss: 0.779254]\n",
            "435 [D loss: 0.661628, acc.: 0.00%] [G loss: 0.805745]\n",
            "436 [D loss: 0.670701, acc.: 0.00%] [G loss: 0.786661]\n",
            "437 [D loss: 0.659312, acc.: 0.00%] [G loss: 0.804445]\n",
            "438 [D loss: 0.660730, acc.: 0.00%] [G loss: 0.789305]\n",
            "439 [D loss: 0.659079, acc.: 0.00%] [G loss: 0.797598]\n",
            "440 [D loss: 0.660776, acc.: 0.00%] [G loss: 0.807354]\n",
            "441 [D loss: 0.655580, acc.: 0.00%] [G loss: 0.822676]\n",
            "442 [D loss: 0.654300, acc.: 0.00%] [G loss: 0.831435]\n",
            "443 [D loss: 0.653483, acc.: 0.00%] [G loss: 0.820228]\n",
            "444 [D loss: 0.654278, acc.: 0.00%] [G loss: 0.829247]\n",
            "445 [D loss: 0.653043, acc.: 0.00%] [G loss: 0.840541]\n",
            "446 [D loss: 0.654791, acc.: 0.00%] [G loss: 0.816609]\n",
            "447 [D loss: 0.652281, acc.: 0.00%] [G loss: 0.812513]\n",
            "448 [D loss: 0.655225, acc.: 0.00%] [G loss: 0.828846]\n",
            "449 [D loss: 0.654718, acc.: 0.00%] [G loss: 0.827275]\n",
            "450 [D loss: 0.657276, acc.: 0.00%] [G loss: 0.819489]\n",
            "451 [D loss: 0.657902, acc.: 0.00%] [G loss: 0.838402]\n",
            "452 [D loss: 0.662492, acc.: 0.00%] [G loss: 0.819697]\n",
            "453 [D loss: 0.654532, acc.: 0.00%] [G loss: 0.828121]\n",
            "454 [D loss: 0.656757, acc.: 0.00%] [G loss: 0.789008]\n",
            "455 [D loss: 0.662140, acc.: 0.00%] [G loss: 0.829497]\n",
            "456 [D loss: 0.666249, acc.: 0.00%] [G loss: 0.807271]\n",
            "457 [D loss: 0.661542, acc.: 0.00%] [G loss: 0.811789]\n",
            "458 [D loss: 0.666853, acc.: 0.00%] [G loss: 0.787036]\n",
            "459 [D loss: 0.661785, acc.: 0.00%] [G loss: 0.824410]\n",
            "460 [D loss: 0.670206, acc.: 0.00%] [G loss: 0.779190]\n",
            "461 [D loss: 0.664530, acc.: 0.00%] [G loss: 0.814269]\n",
            "462 [D loss: 0.668100, acc.: 0.00%] [G loss: 0.823964]\n",
            "463 [D loss: 0.658487, acc.: 0.00%] [G loss: 0.839192]\n",
            "464 [D loss: 0.666868, acc.: 0.00%] [G loss: 0.820661]\n",
            "465 [D loss: 0.669528, acc.: 0.00%] [G loss: 0.844346]\n",
            "466 [D loss: 0.692573, acc.: 0.00%] [G loss: 0.825801]\n",
            "467 [D loss: 0.666759, acc.: 0.00%] [G loss: 0.838547]\n",
            "468 [D loss: 0.679397, acc.: 0.00%] [G loss: 0.807946]\n",
            "469 [D loss: 0.662932, acc.: 0.00%] [G loss: 0.840329]\n",
            "470 [D loss: 0.668067, acc.: 0.00%] [G loss: 0.812184]\n",
            "471 [D loss: 0.660391, acc.: 0.00%] [G loss: 0.845261]\n",
            "472 [D loss: 0.663090, acc.: 0.00%] [G loss: 0.828197]\n",
            "473 [D loss: 0.658906, acc.: 0.00%] [G loss: 0.857859]\n",
            "474 [D loss: 0.661537, acc.: 0.00%] [G loss: 0.817742]\n",
            "475 [D loss: 0.653664, acc.: 0.00%] [G loss: 0.824526]\n",
            "476 [D loss: 0.654656, acc.: 0.00%] [G loss: 0.840525]\n",
            "477 [D loss: 0.655449, acc.: 0.00%] [G loss: 0.828249]\n",
            "478 [D loss: 0.653401, acc.: 0.00%] [G loss: 0.842987]\n",
            "479 [D loss: 0.652871, acc.: 0.00%] [G loss: 0.834705]\n",
            "480 [D loss: 0.651223, acc.: 0.00%] [G loss: 0.836764]\n",
            "481 [D loss: 0.652048, acc.: 0.00%] [G loss: 0.835762]\n",
            "482 [D loss: 0.652081, acc.: 0.00%] [G loss: 0.830265]\n",
            "483 [D loss: 0.654175, acc.: 0.00%] [G loss: 0.848197]\n",
            "484 [D loss: 0.657850, acc.: 0.00%] [G loss: 0.823623]\n",
            "485 [D loss: 0.656716, acc.: 0.00%] [G loss: 0.835851]\n",
            "486 [D loss: 0.662440, acc.: 0.00%] [G loss: 0.828866]\n",
            "487 [D loss: 0.661028, acc.: 0.00%] [G loss: 0.843421]\n",
            "488 [D loss: 0.665555, acc.: 0.00%] [G loss: 0.833476]\n",
            "489 [D loss: 0.659241, acc.: 0.00%] [G loss: 0.845174]\n",
            "490 [D loss: 0.668169, acc.: 0.00%] [G loss: 0.820292]\n",
            "491 [D loss: 0.664363, acc.: 0.00%] [G loss: 0.855646]\n",
            "492 [D loss: 0.668304, acc.: 0.00%] [G loss: 0.823793]\n",
            "493 [D loss: 0.660942, acc.: 0.00%] [G loss: 0.835572]\n",
            "494 [D loss: 0.666915, acc.: 0.00%] [G loss: 0.831737]\n",
            "495 [D loss: 0.658588, acc.: 0.00%] [G loss: 0.850408]\n",
            "496 [D loss: 0.666774, acc.: 0.00%] [G loss: 0.817597]\n",
            "497 [D loss: 0.667742, acc.: 0.00%] [G loss: 0.838269]\n",
            "498 [D loss: 0.690712, acc.: 0.00%] [G loss: 0.820105]\n",
            "499 [D loss: 0.659481, acc.: 0.00%] [G loss: 0.831334]\n",
            "500 [D loss: 0.660890, acc.: 0.00%] [G loss: 0.827983]\n",
            "501 [D loss: 0.661736, acc.: 0.00%] [G loss: 0.868498]\n",
            "502 [D loss: 0.674436, acc.: 0.00%] [G loss: 0.796600]\n",
            "503 [D loss: 0.661524, acc.: 0.00%] [G loss: 0.810720]\n",
            "504 [D loss: 0.659467, acc.: 0.00%] [G loss: 0.816171]\n",
            "505 [D loss: 0.662147, acc.: 0.00%] [G loss: 0.851386]\n",
            "506 [D loss: 0.670735, acc.: 0.00%] [G loss: 0.822679]\n",
            "507 [D loss: 0.655119, acc.: 0.00%] [G loss: 0.833693]\n",
            "508 [D loss: 0.656225, acc.: 0.00%] [G loss: 0.861988]\n",
            "509 [D loss: 0.655403, acc.: 0.00%] [G loss: 0.831648]\n",
            "510 [D loss: 0.651514, acc.: 0.00%] [G loss: 0.843863]\n",
            "511 [D loss: 0.652040, acc.: 0.00%] [G loss: 0.815674]\n",
            "512 [D loss: 0.653029, acc.: 0.00%] [G loss: 0.806311]\n",
            "513 [D loss: 0.657456, acc.: 0.00%] [G loss: 0.799147]\n",
            "514 [D loss: 0.655435, acc.: 0.00%] [G loss: 0.819972]\n",
            "515 [D loss: 0.657744, acc.: 0.00%] [G loss: 0.807006]\n",
            "516 [D loss: 0.657240, acc.: 0.00%] [G loss: 0.836229]\n",
            "517 [D loss: 0.658939, acc.: 0.00%] [G loss: 0.811348]\n",
            "518 [D loss: 0.655213, acc.: 0.00%] [G loss: 0.827945]\n",
            "519 [D loss: 0.654592, acc.: 0.00%] [G loss: 0.838123]\n",
            "520 [D loss: 0.656803, acc.: 0.00%] [G loss: 0.834829]\n",
            "521 [D loss: 0.657046, acc.: 0.00%] [G loss: 0.836319]\n",
            "522 [D loss: 0.654380, acc.: 0.00%] [G loss: 0.848744]\n",
            "523 [D loss: 0.656297, acc.: 0.00%] [G loss: 0.799457]\n",
            "524 [D loss: 0.655471, acc.: 0.00%] [G loss: 0.807442]\n",
            "525 [D loss: 0.652184, acc.: 0.00%] [G loss: 0.816958]\n",
            "526 [D loss: 0.651855, acc.: 0.00%] [G loss: 0.816614]\n",
            "527 [D loss: 0.652023, acc.: 0.00%] [G loss: 0.813268]\n",
            "528 [D loss: 0.653600, acc.: 0.00%] [G loss: 0.824249]\n",
            "529 [D loss: 0.654553, acc.: 0.00%] [G loss: 0.802241]\n",
            "530 [D loss: 0.654143, acc.: 0.00%] [G loss: 0.809258]\n",
            "531 [D loss: 0.655742, acc.: 0.00%] [G loss: 0.816841]\n",
            "532 [D loss: 0.659181, acc.: 0.00%] [G loss: 0.836431]\n",
            "533 [D loss: 0.671188, acc.: 0.00%] [G loss: 0.814242]\n",
            "534 [D loss: 0.659318, acc.: 0.00%] [G loss: 0.843618]\n",
            "535 [D loss: 0.660256, acc.: 0.00%] [G loss: 0.823465]\n",
            "536 [D loss: 0.664851, acc.: 0.00%] [G loss: 0.824409]\n",
            "537 [D loss: 0.682888, acc.: 0.00%] [G loss: 0.781644]\n",
            "538 [D loss: 0.667520, acc.: 0.00%] [G loss: 0.813114]\n",
            "539 [D loss: 0.674201, acc.: 0.00%] [G loss: 0.795310]\n",
            "540 [D loss: 0.655931, acc.: 0.00%] [G loss: 0.805816]\n",
            "541 [D loss: 0.653833, acc.: 0.00%] [G loss: 0.806590]\n",
            "542 [D loss: 0.654988, acc.: 0.00%] [G loss: 0.829797]\n",
            "543 [D loss: 0.659180, acc.: 0.00%] [G loss: 0.818080]\n",
            "544 [D loss: 0.654309, acc.: 0.00%] [G loss: 0.829922]\n",
            "545 [D loss: 0.655994, acc.: 0.00%] [G loss: 0.832802]\n",
            "546 [D loss: 0.657497, acc.: 0.00%] [G loss: 0.858369]\n",
            "547 [D loss: 0.671560, acc.: 0.00%] [G loss: 0.823022]\n",
            "548 [D loss: 0.669840, acc.: 0.00%] [G loss: 0.844239]\n",
            "549 [D loss: 0.694401, acc.: 0.00%] [G loss: 0.819533]\n",
            "550 [D loss: 0.657793, acc.: 0.00%] [G loss: 0.819121]\n",
            "551 [D loss: 0.656581, acc.: 0.00%] [G loss: 0.791329]\n",
            "552 [D loss: 0.656849, acc.: 0.00%] [G loss: 0.814809]\n",
            "553 [D loss: 0.656109, acc.: 0.00%] [G loss: 0.794088]\n",
            "554 [D loss: 0.652853, acc.: 0.00%] [G loss: 0.794591]\n",
            "555 [D loss: 0.652130, acc.: 0.00%] [G loss: 0.791572]\n",
            "556 [D loss: 0.652615, acc.: 0.00%] [G loss: 0.787795]\n",
            "557 [D loss: 0.657744, acc.: 0.00%] [G loss: 0.814461]\n",
            "558 [D loss: 0.666342, acc.: 0.00%] [G loss: 0.794866]\n",
            "559 [D loss: 0.662298, acc.: 0.00%] [G loss: 0.823109]\n",
            "560 [D loss: 0.674538, acc.: 0.00%] [G loss: 0.800990]\n",
            "561 [D loss: 0.666278, acc.: 0.00%] [G loss: 0.820746]\n",
            "562 [D loss: 0.677817, acc.: 0.00%] [G loss: 0.807009]\n",
            "563 [D loss: 0.656775, acc.: 0.00%] [G loss: 0.819110]\n",
            "564 [D loss: 0.656473, acc.: 0.00%] [G loss: 0.814426]\n",
            "565 [D loss: 0.653394, acc.: 0.00%] [G loss: 0.838890]\n",
            "566 [D loss: 0.652971, acc.: 0.00%] [G loss: 0.829367]\n",
            "567 [D loss: 0.654234, acc.: 0.00%] [G loss: 0.842796]\n",
            "568 [D loss: 0.658737, acc.: 0.00%] [G loss: 0.820813]\n",
            "569 [D loss: 0.660544, acc.: 0.00%] [G loss: 0.861048]\n",
            "570 [D loss: 0.659982, acc.: 0.00%] [G loss: 0.832558]\n",
            "571 [D loss: 0.662207, acc.: 0.00%] [G loss: 0.854927]\n",
            "572 [D loss: 0.671159, acc.: 0.00%] [G loss: 0.847099]\n",
            "573 [D loss: 0.653685, acc.: 0.00%] [G loss: 0.838769]\n",
            "574 [D loss: 0.651590, acc.: 0.00%] [G loss: 0.836158]\n",
            "575 [D loss: 0.650817, acc.: 0.00%] [G loss: 0.829163]\n",
            "576 [D loss: 0.653320, acc.: 0.00%] [G loss: 0.852376]\n",
            "577 [D loss: 0.653616, acc.: 0.00%] [G loss: 0.859555]\n",
            "578 [D loss: 0.657210, acc.: 0.00%] [G loss: 0.857340]\n",
            "579 [D loss: 0.660506, acc.: 0.00%] [G loss: 0.856518]\n",
            "580 [D loss: 0.673390, acc.: 0.00%] [G loss: 0.825975]\n",
            "581 [D loss: 0.662576, acc.: 0.00%] [G loss: 0.837917]\n",
            "582 [D loss: 0.670852, acc.: 0.00%] [G loss: 0.836853]\n",
            "583 [D loss: 0.663307, acc.: 0.00%] [G loss: 0.836189]\n",
            "584 [D loss: 0.665728, acc.: 0.00%] [G loss: 0.819141]\n",
            "585 [D loss: 0.659352, acc.: 0.00%] [G loss: 0.846448]\n",
            "586 [D loss: 0.662617, acc.: 0.00%] [G loss: 0.818769]\n",
            "587 [D loss: 0.659868, acc.: 0.00%] [G loss: 0.835843]\n",
            "588 [D loss: 0.660009, acc.: 0.00%] [G loss: 0.828203]\n",
            "589 [D loss: 0.654774, acc.: 0.00%] [G loss: 0.848737]\n",
            "590 [D loss: 0.653482, acc.: 0.00%] [G loss: 0.823400]\n",
            "591 [D loss: 0.655481, acc.: 0.00%] [G loss: 0.833020]\n",
            "592 [D loss: 0.662658, acc.: 0.00%] [G loss: 0.818924]\n",
            "593 [D loss: 0.658426, acc.: 0.00%] [G loss: 0.833411]\n",
            "594 [D loss: 0.657480, acc.: 0.00%] [G loss: 0.823781]\n",
            "595 [D loss: 0.653859, acc.: 0.00%] [G loss: 0.849909]\n",
            "596 [D loss: 0.652985, acc.: 0.00%] [G loss: 0.834185]\n",
            "597 [D loss: 0.653328, acc.: 0.00%] [G loss: 0.846743]\n",
            "598 [D loss: 0.653373, acc.: 0.00%] [G loss: 0.842461]\n",
            "599 [D loss: 0.652181, acc.: 0.00%] [G loss: 0.838817]\n",
            "600 [D loss: 0.652984, acc.: 0.00%] [G loss: 0.839152]\n",
            "601 [D loss: 0.654256, acc.: 0.00%] [G loss: 0.818816]\n",
            "602 [D loss: 0.655149, acc.: 0.00%] [G loss: 0.827736]\n",
            "603 [D loss: 0.657039, acc.: 0.00%] [G loss: 0.836305]\n",
            "604 [D loss: 0.658967, acc.: 0.00%] [G loss: 0.823465]\n",
            "605 [D loss: 0.656990, acc.: 0.00%] [G loss: 0.849461]\n",
            "606 [D loss: 0.658683, acc.: 0.00%] [G loss: 0.825794]\n",
            "607 [D loss: 0.655170, acc.: 0.00%] [G loss: 0.833674]\n",
            "608 [D loss: 0.653435, acc.: 0.00%] [G loss: 0.833755]\n",
            "609 [D loss: 0.653241, acc.: 0.00%] [G loss: 0.828060]\n",
            "610 [D loss: 0.652557, acc.: 0.00%] [G loss: 0.825413]\n",
            "611 [D loss: 0.651641, acc.: 0.00%] [G loss: 0.834635]\n",
            "612 [D loss: 0.652079, acc.: 0.00%] [G loss: 0.829791]\n",
            "613 [D loss: 0.653701, acc.: 0.00%] [G loss: 0.831576]\n",
            "614 [D loss: 0.657332, acc.: 0.00%] [G loss: 0.862458]\n",
            "615 [D loss: 0.667451, acc.: 0.00%] [G loss: 0.852061]\n",
            "616 [D loss: 0.662994, acc.: 0.00%] [G loss: 0.888232]\n",
            "617 [D loss: 0.671604, acc.: 0.00%] [G loss: 0.855150]\n",
            "618 [D loss: 0.661085, acc.: 0.00%] [G loss: 0.879483]\n",
            "619 [D loss: 0.654094, acc.: 0.00%] [G loss: 0.853871]\n",
            "620 [D loss: 0.653254, acc.: 0.00%] [G loss: 0.867867]\n",
            "621 [D loss: 0.652454, acc.: 0.00%] [G loss: 0.855291]\n",
            "622 [D loss: 0.651222, acc.: 0.00%] [G loss: 0.849307]\n",
            "623 [D loss: 0.651375, acc.: 0.00%] [G loss: 0.847133]\n",
            "624 [D loss: 0.650632, acc.: 0.00%] [G loss: 0.841879]\n",
            "625 [D loss: 0.651253, acc.: 0.00%] [G loss: 0.857353]\n",
            "626 [D loss: 0.652431, acc.: 0.00%] [G loss: 0.851116]\n",
            "627 [D loss: 0.654856, acc.: 0.00%] [G loss: 0.860435]\n",
            "628 [D loss: 0.664338, acc.: 0.00%] [G loss: 0.851183]\n",
            "629 [D loss: 0.658439, acc.: 0.00%] [G loss: 0.879621]\n",
            "630 [D loss: 0.658656, acc.: 0.00%] [G loss: 0.857640]\n",
            "631 [D loss: 0.656969, acc.: 0.00%] [G loss: 0.843074]\n",
            "632 [D loss: 0.659040, acc.: 0.00%] [G loss: 0.832213]\n",
            "633 [D loss: 0.658018, acc.: 0.00%] [G loss: 0.869642]\n",
            "634 [D loss: 0.661754, acc.: 0.00%] [G loss: 0.849124]\n",
            "635 [D loss: 0.659443, acc.: 0.00%] [G loss: 0.881413]\n",
            "636 [D loss: 0.663227, acc.: 0.00%] [G loss: 0.844614]\n",
            "637 [D loss: 0.657028, acc.: 0.00%] [G loss: 0.854569]\n",
            "638 [D loss: 0.659374, acc.: 0.00%] [G loss: 0.858248]\n",
            "639 [D loss: 0.659393, acc.: 0.00%] [G loss: 0.847072]\n",
            "640 [D loss: 0.663212, acc.: 0.00%] [G loss: 0.823624]\n",
            "641 [D loss: 0.656745, acc.: 0.00%] [G loss: 0.850226]\n",
            "642 [D loss: 0.654993, acc.: 0.00%] [G loss: 0.843705]\n",
            "643 [D loss: 0.658123, acc.: 0.00%] [G loss: 0.840185]\n",
            "644 [D loss: 0.665210, acc.: 0.00%] [G loss: 0.851705]\n",
            "645 [D loss: 0.655110, acc.: 0.00%] [G loss: 0.864004]\n",
            "646 [D loss: 0.655400, acc.: 0.00%] [G loss: 0.856186]\n",
            "647 [D loss: 0.656183, acc.: 0.00%] [G loss: 0.868981]\n",
            "648 [D loss: 0.657739, acc.: 0.00%] [G loss: 0.865644]\n",
            "649 [D loss: 0.655287, acc.: 0.00%] [G loss: 0.870074]\n",
            "650 [D loss: 0.656157, acc.: 0.00%] [G loss: 0.852665]\n",
            "651 [D loss: 0.652233, acc.: 0.00%] [G loss: 0.854858]\n",
            "652 [D loss: 0.651836, acc.: 0.00%] [G loss: 0.849858]\n",
            "653 [D loss: 0.652935, acc.: 0.00%] [G loss: 0.860236]\n",
            "654 [D loss: 0.655659, acc.: 0.00%] [G loss: 0.850468]\n",
            "655 [D loss: 0.657433, acc.: 0.00%] [G loss: 0.874394]\n",
            "656 [D loss: 0.665079, acc.: 0.00%] [G loss: 0.868615]\n",
            "657 [D loss: 0.657255, acc.: 0.00%] [G loss: 0.894575]\n",
            "658 [D loss: 0.655424, acc.: 0.00%] [G loss: 0.875603]\n",
            "659 [D loss: 0.652907, acc.: 0.00%] [G loss: 0.867214]\n",
            "660 [D loss: 0.652834, acc.: 0.00%] [G loss: 0.869593]\n",
            "661 [D loss: 0.652147, acc.: 0.00%] [G loss: 0.881518]\n",
            "662 [D loss: 0.653122, acc.: 0.00%] [G loss: 0.873132]\n",
            "663 [D loss: 0.651203, acc.: 0.00%] [G loss: 0.884766]\n",
            "664 [D loss: 0.651550, acc.: 0.00%] [G loss: 0.890048]\n",
            "665 [D loss: 0.650914, acc.: 0.00%] [G loss: 0.886373]\n",
            "666 [D loss: 0.651155, acc.: 0.00%] [G loss: 0.868014]\n",
            "667 [D loss: 0.652194, acc.: 0.00%] [G loss: 0.882504]\n",
            "668 [D loss: 0.652605, acc.: 0.00%] [G loss: 0.866085]\n",
            "669 [D loss: 0.651336, acc.: 0.00%] [G loss: 0.868267]\n",
            "670 [D loss: 0.651587, acc.: 0.00%] [G loss: 0.862509]\n",
            "671 [D loss: 0.653116, acc.: 0.00%] [G loss: 0.866662]\n",
            "672 [D loss: 0.656363, acc.: 0.00%] [G loss: 0.864414]\n",
            "673 [D loss: 0.656879, acc.: 0.00%] [G loss: 0.896068]\n",
            "674 [D loss: 0.660962, acc.: 0.00%] [G loss: 0.865269]\n",
            "675 [D loss: 0.659795, acc.: 0.00%] [G loss: 0.880608]\n",
            "676 [D loss: 0.664825, acc.: 0.00%] [G loss: 0.839288]\n",
            "677 [D loss: 0.660008, acc.: 0.00%] [G loss: 0.874610]\n",
            "678 [D loss: 0.659559, acc.: 0.00%] [G loss: 0.858062]\n",
            "679 [D loss: 0.654049, acc.: 0.00%] [G loss: 0.868473]\n",
            "680 [D loss: 0.656731, acc.: 0.00%] [G loss: 0.871868]\n",
            "681 [D loss: 0.655455, acc.: 0.00%] [G loss: 0.885615]\n",
            "682 [D loss: 0.655239, acc.: 0.00%] [G loss: 0.877618]\n",
            "683 [D loss: 0.653038, acc.: 0.00%] [G loss: 0.869117]\n",
            "684 [D loss: 0.654180, acc.: 0.00%] [G loss: 0.869311]\n",
            "685 [D loss: 0.652851, acc.: 0.00%] [G loss: 0.883024]\n",
            "686 [D loss: 0.654833, acc.: 0.00%] [G loss: 0.857679]\n",
            "687 [D loss: 0.654553, acc.: 0.00%] [G loss: 0.880135]\n",
            "688 [D loss: 0.654279, acc.: 0.00%] [G loss: 0.868841]\n",
            "689 [D loss: 0.654203, acc.: 0.00%] [G loss: 0.863666]\n",
            "690 [D loss: 0.661008, acc.: 0.00%] [G loss: 0.861608]\n",
            "691 [D loss: 0.662807, acc.: 0.00%] [G loss: 0.864302]\n",
            "692 [D loss: 0.667604, acc.: 0.00%] [G loss: 0.850699]\n",
            "693 [D loss: 0.657612, acc.: 0.00%] [G loss: 0.860422]\n",
            "694 [D loss: 0.659019, acc.: 0.00%] [G loss: 0.852784]\n",
            "695 [D loss: 0.654867, acc.: 0.00%] [G loss: 0.872794]\n",
            "696 [D loss: 0.654538, acc.: 0.00%] [G loss: 0.861798]\n",
            "697 [D loss: 0.653916, acc.: 0.00%] [G loss: 0.885291]\n",
            "698 [D loss: 0.654660, acc.: 0.00%] [G loss: 0.871763]\n",
            "699 [D loss: 0.654153, acc.: 0.00%] [G loss: 0.885410]\n",
            "700 [D loss: 0.657011, acc.: 0.00%] [G loss: 0.871878]\n",
            "701 [D loss: 0.658783, acc.: 0.00%] [G loss: 0.902125]\n",
            "702 [D loss: 0.663079, acc.: 0.00%] [G loss: 0.870602]\n",
            "703 [D loss: 0.658085, acc.: 0.00%] [G loss: 0.867850]\n",
            "704 [D loss: 0.654890, acc.: 0.00%] [G loss: 0.859059]\n",
            "705 [D loss: 0.652765, acc.: 0.00%] [G loss: 0.878517]\n",
            "706 [D loss: 0.651954, acc.: 0.00%] [G loss: 0.880319]\n",
            "707 [D loss: 0.653761, acc.: 0.00%] [G loss: 0.870535]\n",
            "708 [D loss: 0.655612, acc.: 0.00%] [G loss: 0.844917]\n",
            "709 [D loss: 0.654411, acc.: 0.00%] [G loss: 0.852602]\n",
            "710 [D loss: 0.656026, acc.: 0.00%] [G loss: 0.836929]\n",
            "711 [D loss: 0.656510, acc.: 0.00%] [G loss: 0.853596]\n",
            "712 [D loss: 0.658284, acc.: 0.00%] [G loss: 0.853324]\n",
            "713 [D loss: 0.654368, acc.: 0.00%] [G loss: 0.856511]\n",
            "714 [D loss: 0.654523, acc.: 0.00%] [G loss: 0.849074]\n",
            "715 [D loss: 0.653760, acc.: 0.00%] [G loss: 0.865404]\n",
            "716 [D loss: 0.656730, acc.: 0.00%] [G loss: 0.862175]\n",
            "717 [D loss: 0.659307, acc.: 0.00%] [G loss: 0.852033]\n",
            "718 [D loss: 0.659753, acc.: 0.00%] [G loss: 0.859802]\n",
            "719 [D loss: 0.654350, acc.: 0.00%] [G loss: 0.881706]\n",
            "720 [D loss: 0.652453, acc.: 0.00%] [G loss: 0.863336]\n",
            "721 [D loss: 0.652713, acc.: 0.00%] [G loss: 0.857115]\n",
            "722 [D loss: 0.654575, acc.: 0.00%] [G loss: 0.837424]\n",
            "723 [D loss: 0.654201, acc.: 0.00%] [G loss: 0.866277]\n",
            "724 [D loss: 0.652842, acc.: 0.00%] [G loss: 0.864527]\n",
            "725 [D loss: 0.652520, acc.: 0.00%] [G loss: 0.888174]\n",
            "726 [D loss: 0.653262, acc.: 0.00%] [G loss: 0.855107]\n",
            "727 [D loss: 0.652141, acc.: 0.00%] [G loss: 0.862754]\n",
            "728 [D loss: 0.653129, acc.: 0.00%] [G loss: 0.850198]\n",
            "729 [D loss: 0.653210, acc.: 0.00%] [G loss: 0.869407]\n",
            "730 [D loss: 0.655684, acc.: 0.00%] [G loss: 0.852911]\n",
            "731 [D loss: 0.660761, acc.: 0.00%] [G loss: 0.888212]\n",
            "732 [D loss: 0.674002, acc.: 0.00%] [G loss: 0.840380]\n",
            "733 [D loss: 0.664820, acc.: 0.00%] [G loss: 0.862251]\n",
            "734 [D loss: 0.677629, acc.: 0.00%] [G loss: 0.844370]\n",
            "735 [D loss: 0.667841, acc.: 0.00%] [G loss: 0.878024]\n",
            "736 [D loss: 0.685126, acc.: 0.00%] [G loss: 0.859469]\n",
            "737 [D loss: 0.658633, acc.: 0.00%] [G loss: 0.870205]\n",
            "738 [D loss: 0.661707, acc.: 0.00%] [G loss: 0.872668]\n",
            "739 [D loss: 0.656276, acc.: 0.00%] [G loss: 0.871547]\n",
            "740 [D loss: 0.656770, acc.: 0.00%] [G loss: 0.860163]\n",
            "741 [D loss: 0.654190, acc.: 0.00%] [G loss: 0.878713]\n",
            "742 [D loss: 0.655271, acc.: 0.00%] [G loss: 0.845879]\n",
            "743 [D loss: 0.655657, acc.: 0.00%] [G loss: 0.869431]\n",
            "744 [D loss: 0.656479, acc.: 0.00%] [G loss: 0.844371]\n",
            "745 [D loss: 0.653991, acc.: 0.00%] [G loss: 0.865082]\n",
            "746 [D loss: 0.653704, acc.: 0.00%] [G loss: 0.858931]\n",
            "747 [D loss: 0.656127, acc.: 0.00%] [G loss: 0.870266]\n",
            "748 [D loss: 0.663331, acc.: 0.00%] [G loss: 0.850775]\n",
            "749 [D loss: 0.664182, acc.: 0.00%] [G loss: 0.888512]\n",
            "750 [D loss: 0.673170, acc.: 0.00%] [G loss: 0.853968]\n",
            "751 [D loss: 0.659210, acc.: 0.00%] [G loss: 0.865628]\n",
            "752 [D loss: 0.655977, acc.: 0.00%] [G loss: 0.857136]\n",
            "753 [D loss: 0.654949, acc.: 0.00%] [G loss: 0.870687]\n",
            "754 [D loss: 0.655292, acc.: 0.00%] [G loss: 0.853883]\n",
            "755 [D loss: 0.654150, acc.: 0.00%] [G loss: 0.851108]\n",
            "756 [D loss: 0.658998, acc.: 0.00%] [G loss: 0.843798]\n",
            "757 [D loss: 0.656222, acc.: 0.00%] [G loss: 0.868223]\n",
            "758 [D loss: 0.656592, acc.: 0.00%] [G loss: 0.865955]\n",
            "759 [D loss: 0.656060, acc.: 0.00%] [G loss: 0.875526]\n",
            "760 [D loss: 0.657969, acc.: 0.00%] [G loss: 0.856088]\n",
            "761 [D loss: 0.656509, acc.: 0.00%] [G loss: 0.873238]\n",
            "762 [D loss: 0.657703, acc.: 0.00%] [G loss: 0.847875]\n",
            "763 [D loss: 0.653654, acc.: 0.00%] [G loss: 0.858045]\n",
            "764 [D loss: 0.652574, acc.: 0.00%] [G loss: 0.856111]\n",
            "765 [D loss: 0.653059, acc.: 0.00%] [G loss: 0.857255]\n",
            "766 [D loss: 0.652489, acc.: 0.00%] [G loss: 0.854048]\n",
            "767 [D loss: 0.653469, acc.: 0.00%] [G loss: 0.872996]\n",
            "768 [D loss: 0.653145, acc.: 0.00%] [G loss: 0.860944]\n",
            "769 [D loss: 0.651997, acc.: 0.00%] [G loss: 0.851398]\n",
            "770 [D loss: 0.654578, acc.: 0.00%] [G loss: 0.854393]\n",
            "771 [D loss: 0.654064, acc.: 0.00%] [G loss: 0.852511]\n",
            "772 [D loss: 0.655349, acc.: 0.00%] [G loss: 0.840782]\n",
            "773 [D loss: 0.654918, acc.: 0.00%] [G loss: 0.858691]\n",
            "774 [D loss: 0.658171, acc.: 0.00%] [G loss: 0.834396]\n",
            "775 [D loss: 0.655057, acc.: 0.00%] [G loss: 0.830058]\n",
            "776 [D loss: 0.658460, acc.: 0.00%] [G loss: 0.832650]\n",
            "777 [D loss: 0.658223, acc.: 0.00%] [G loss: 0.854110]\n",
            "778 [D loss: 0.659753, acc.: 0.00%] [G loss: 0.865095]\n",
            "779 [D loss: 0.651584, acc.: 0.00%] [G loss: 0.866340]\n",
            "780 [D loss: 0.651295, acc.: 0.00%] [G loss: 0.870305]\n",
            "781 [D loss: 0.652488, acc.: 0.00%] [G loss: 0.842130]\n",
            "782 [D loss: 0.653248, acc.: 0.00%] [G loss: 0.862644]\n",
            "783 [D loss: 0.652453, acc.: 0.00%] [G loss: 0.851260]\n",
            "784 [D loss: 0.652310, acc.: 0.00%] [G loss: 0.858782]\n",
            "785 [D loss: 0.653360, acc.: 0.00%] [G loss: 0.841603]\n",
            "786 [D loss: 0.652529, acc.: 0.00%] [G loss: 0.861892]\n",
            "787 [D loss: 0.652906, acc.: 0.00%] [G loss: 0.841627]\n",
            "788 [D loss: 0.653681, acc.: 0.00%] [G loss: 0.835381]\n",
            "789 [D loss: 0.655086, acc.: 0.00%] [G loss: 0.847391]\n",
            "790 [D loss: 0.653276, acc.: 0.00%] [G loss: 0.870638]\n",
            "791 [D loss: 0.654081, acc.: 0.00%] [G loss: 0.862793]\n",
            "792 [D loss: 0.653629, acc.: 0.00%] [G loss: 0.885884]\n",
            "793 [D loss: 0.652735, acc.: 0.00%] [G loss: 0.863439]\n",
            "794 [D loss: 0.653898, acc.: 0.00%] [G loss: 0.887680]\n",
            "795 [D loss: 0.658629, acc.: 0.00%] [G loss: 0.866321]\n",
            "796 [D loss: 0.660308, acc.: 0.00%] [G loss: 0.899889]\n",
            "797 [D loss: 0.671810, acc.: 0.00%] [G loss: 0.855514]\n",
            "798 [D loss: 0.662464, acc.: 0.00%] [G loss: 0.881869]\n",
            "799 [D loss: 0.659160, acc.: 0.00%] [G loss: 0.875805]\n",
            "800 [D loss: 0.653992, acc.: 0.00%] [G loss: 0.871973]\n",
            "801 [D loss: 0.652804, acc.: 0.00%] [G loss: 0.874783]\n",
            "802 [D loss: 0.651765, acc.: 0.00%] [G loss: 0.871482]\n",
            "803 [D loss: 0.652177, acc.: 0.00%] [G loss: 0.851439]\n",
            "804 [D loss: 0.652377, acc.: 0.00%] [G loss: 0.876777]\n",
            "805 [D loss: 0.652178, acc.: 0.00%] [G loss: 0.865372]\n",
            "806 [D loss: 0.651876, acc.: 0.00%] [G loss: 0.869939]\n",
            "807 [D loss: 0.652355, acc.: 0.00%] [G loss: 0.892576]\n",
            "808 [D loss: 0.655522, acc.: 0.00%] [G loss: 0.873170]\n",
            "809 [D loss: 0.651406, acc.: 0.00%] [G loss: 0.877117]\n",
            "810 [D loss: 0.651998, acc.: 0.00%] [G loss: 0.864242]\n",
            "811 [D loss: 0.650702, acc.: 0.00%] [G loss: 0.859591]\n",
            "812 [D loss: 0.651617, acc.: 0.00%] [G loss: 0.837371]\n",
            "813 [D loss: 0.652056, acc.: 0.00%] [G loss: 0.853040]\n",
            "814 [D loss: 0.652212, acc.: 0.00%] [G loss: 0.856671]\n",
            "815 [D loss: 0.651567, acc.: 0.00%] [G loss: 0.867745]\n",
            "816 [D loss: 0.653182, acc.: 0.00%] [G loss: 0.870588]\n",
            "817 [D loss: 0.652531, acc.: 0.00%] [G loss: 0.880408]\n",
            "818 [D loss: 0.652856, acc.: 0.00%] [G loss: 0.877756]\n",
            "819 [D loss: 0.652884, acc.: 0.00%] [G loss: 0.885107]\n",
            "820 [D loss: 0.654164, acc.: 0.00%] [G loss: 0.873156]\n",
            "821 [D loss: 0.653803, acc.: 0.00%] [G loss: 0.889207]\n",
            "822 [D loss: 0.657513, acc.: 0.00%] [G loss: 0.886510]\n",
            "823 [D loss: 0.657601, acc.: 0.00%] [G loss: 0.875753]\n",
            "824 [D loss: 0.658903, acc.: 0.00%] [G loss: 0.866407]\n",
            "825 [D loss: 0.653862, acc.: 0.00%] [G loss: 0.890972]\n",
            "826 [D loss: 0.652982, acc.: 0.00%] [G loss: 0.870380]\n",
            "827 [D loss: 0.651552, acc.: 0.00%] [G loss: 0.866093]\n",
            "828 [D loss: 0.652833, acc.: 0.00%] [G loss: 0.883413]\n",
            "829 [D loss: 0.650932, acc.: 0.00%] [G loss: 0.898080]\n",
            "830 [D loss: 0.652440, acc.: 0.00%] [G loss: 0.878906]\n",
            "831 [D loss: 0.652619, acc.: 0.00%] [G loss: 0.874885]\n",
            "832 [D loss: 0.651820, acc.: 0.00%] [G loss: 0.870394]\n",
            "833 [D loss: 0.653104, acc.: 0.00%] [G loss: 0.877371]\n",
            "834 [D loss: 0.654044, acc.: 0.00%] [G loss: 0.890564]\n",
            "835 [D loss: 0.655467, acc.: 0.00%] [G loss: 0.886824]\n",
            "836 [D loss: 0.655800, acc.: 0.00%] [G loss: 0.912839]\n",
            "837 [D loss: 0.658845, acc.: 0.00%] [G loss: 0.884653]\n",
            "838 [D loss: 0.655719, acc.: 0.00%] [G loss: 0.900128]\n",
            "839 [D loss: 0.656443, acc.: 0.00%] [G loss: 0.870466]\n",
            "840 [D loss: 0.655540, acc.: 0.00%] [G loss: 0.889686]\n",
            "841 [D loss: 0.658210, acc.: 0.00%] [G loss: 0.876816]\n",
            "842 [D loss: 0.655118, acc.: 0.00%] [G loss: 0.897559]\n",
            "843 [D loss: 0.654967, acc.: 0.00%] [G loss: 0.902170]\n",
            "844 [D loss: 0.655218, acc.: 0.00%] [G loss: 0.891080]\n",
            "845 [D loss: 0.655348, acc.: 0.00%] [G loss: 0.889955]\n",
            "846 [D loss: 0.654933, acc.: 0.00%] [G loss: 0.918883]\n",
            "847 [D loss: 0.654917, acc.: 0.00%] [G loss: 0.875477]\n",
            "848 [D loss: 0.654127, acc.: 0.00%] [G loss: 0.889337]\n",
            "849 [D loss: 0.653615, acc.: 0.00%] [G loss: 0.873182]\n",
            "850 [D loss: 0.653642, acc.: 0.00%] [G loss: 0.895212]\n",
            "851 [D loss: 0.654098, acc.: 0.00%] [G loss: 0.886195]\n",
            "852 [D loss: 0.652118, acc.: 0.00%] [G loss: 0.889916]\n",
            "853 [D loss: 0.651768, acc.: 0.00%] [G loss: 0.885609]\n",
            "854 [D loss: 0.654495, acc.: 0.00%] [G loss: 0.881885]\n",
            "855 [D loss: 0.656898, acc.: 0.00%] [G loss: 0.889528]\n",
            "856 [D loss: 0.654173, acc.: 0.00%] [G loss: 0.903517]\n",
            "857 [D loss: 0.654257, acc.: 0.00%] [G loss: 0.909054]\n",
            "858 [D loss: 0.650779, acc.: 0.00%] [G loss: 0.906781]\n",
            "859 [D loss: 0.651358, acc.: 0.00%] [G loss: 0.899508]\n",
            "860 [D loss: 0.652735, acc.: 0.00%] [G loss: 0.882390]\n",
            "861 [D loss: 0.652313, acc.: 0.00%] [G loss: 0.889748]\n",
            "862 [D loss: 0.653621, acc.: 0.00%] [G loss: 0.890290]\n",
            "863 [D loss: 0.654980, acc.: 0.00%] [G loss: 0.917482]\n",
            "864 [D loss: 0.660457, acc.: 0.00%] [G loss: 0.906737]\n",
            "865 [D loss: 0.659047, acc.: 0.00%] [G loss: 0.922120]\n",
            "866 [D loss: 0.660906, acc.: 0.00%] [G loss: 0.897889]\n",
            "867 [D loss: 0.654031, acc.: 0.00%] [G loss: 0.896453]\n",
            "868 [D loss: 0.652813, acc.: 0.00%] [G loss: 0.896783]\n",
            "869 [D loss: 0.653932, acc.: 0.00%] [G loss: 0.903381]\n",
            "870 [D loss: 0.657957, acc.: 0.00%] [G loss: 0.891719]\n",
            "871 [D loss: 0.656798, acc.: 0.00%] [G loss: 0.914171]\n",
            "872 [D loss: 0.660084, acc.: 0.00%] [G loss: 0.909197]\n",
            "873 [D loss: 0.658306, acc.: 0.00%] [G loss: 0.923460]\n",
            "874 [D loss: 0.661787, acc.: 0.00%] [G loss: 0.895919]\n",
            "875 [D loss: 0.653672, acc.: 0.00%] [G loss: 0.889904]\n",
            "876 [D loss: 0.651673, acc.: 0.00%] [G loss: 0.880124]\n",
            "877 [D loss: 0.650998, acc.: 0.00%] [G loss: 0.889818]\n",
            "878 [D loss: 0.650740, acc.: 0.00%] [G loss: 0.888477]\n",
            "879 [D loss: 0.651067, acc.: 0.00%] [G loss: 0.873324]\n",
            "880 [D loss: 0.651090, acc.: 0.00%] [G loss: 0.870077]\n",
            "881 [D loss: 0.652127, acc.: 0.00%] [G loss: 0.878366]\n",
            "882 [D loss: 0.651214, acc.: 0.00%] [G loss: 0.894346]\n",
            "883 [D loss: 0.651003, acc.: 0.00%] [G loss: 0.888991]\n",
            "884 [D loss: 0.651005, acc.: 0.00%] [G loss: 0.902640]\n",
            "885 [D loss: 0.651166, acc.: 0.00%] [G loss: 0.899655]\n",
            "886 [D loss: 0.652025, acc.: 0.00%] [G loss: 0.888856]\n",
            "887 [D loss: 0.651827, acc.: 0.00%] [G loss: 0.869375]\n",
            "888 [D loss: 0.651584, acc.: 0.00%] [G loss: 0.874754]\n",
            "889 [D loss: 0.651993, acc.: 0.00%] [G loss: 0.882049]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-7446d1e33dcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0mdcgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDCGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0mdcgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-7446d1e33dcb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, save_interval)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;31m# Train the discriminator (real classified as ones and generated as zeros)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0md_loss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0md_loss_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m             \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_loss_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   1346\u001b[0m                                                     class_weight)\n\u001b[1;32m   1347\u001b[0m       \u001b[0mtrain_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOxozbo1EVGG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "60def624-1aae-42c7-deee-2217f7691ffe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}